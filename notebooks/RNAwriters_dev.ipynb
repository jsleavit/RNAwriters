{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c30d1ce-7c86-435c-b72a-8fe83517d95f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Mod enzyme discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae901e1-0cee-4ab2-a2d9-b610fe27462d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class COG_OBJECT():\n",
    "    def __init__(self, cog_source_path):\n",
    "        ''' initialize the files from NCBI cog ftp site'''\n",
    "        self.cog_source_path = cog_source_path\n",
    "        for i in os.listdir(cog_source_path):\n",
    "            if i.startswith('cog-20'):\n",
    "               # self.taxa = i.split('.')[0]+'.tax.csv' # this file doesn't have useful info\n",
    "                self.cog = i.split('.')[0]+'.cog.csv'\n",
    "                self.cog_prod_g = i.split('.')[0]+'.def.tab'\n",
    "                self.cog_org = i.split('.')[0]+'.org.csv'\n",
    "                self.cog_seq = i.split('.')[0]+'.fa.gz'\n",
    "\n",
    "        self.known_files = [self.cog,\n",
    "                            self.cog_prod_g, self.cog_org, self.cog_seq]\n",
    "\n",
    "    def flatten_json(self, y):\n",
    "        ''' flatten json to make it easier to work with '''\n",
    "        out = {}\n",
    "\n",
    "        def flatten(x, name=''):\n",
    "            if type(x) is dict:\n",
    "                for a in x:\n",
    "                    flatten(x[a], name + a + '_')\n",
    "            elif type(x) is list:\n",
    "                i = 0\n",
    "                for a in x:\n",
    "                    flatten(a, name + str(i) + '_')\n",
    "                    i += 1\n",
    "            else:\n",
    "                out[name[:-1]] = x\n",
    "        flatten(y)\n",
    "        self.flat_cog_json = out\n",
    "\n",
    "    def build_cog_json(self, flatten=False, genomes=False):\n",
    "        ''' organize each file and its contents into a json, if genomes is true, then reutrn a dict with genomes by clade, if flatten is true, then flatten the json '''\n",
    "\n",
    "        self.cog_json = {}\n",
    "        self.clade_assemblies = {}\n",
    "        for i in self.known_files:\n",
    "            self.cog_json[i] = {'features': []}\n",
    "\n",
    "            # read in file type by its extension\n",
    "            if i.endswith('.gz'):\n",
    "                with gzip.open(self.cog_source_path+i, 'rt') as f:\n",
    "\n",
    "                    # read in like a fasta file\n",
    "                    for line in f:\n",
    "                        if line.startswith('>'):\n",
    "                            gene_id = line.split()[0][1:]\n",
    "\n",
    "                            # if not a NoneType then add string in between [] to json\n",
    "                            if re.search('\\[.*\\]', line):\n",
    "                                taxid = re.search('\\[(.*?)\\]', line).group(1)\n",
    "\n",
    "                        # while not a new gene_id, add sequence to gene_id\n",
    "                        while not line.startswith('>'):\n",
    "                            self.cog_json[i]['features'].append(\n",
    "                                {'genbank_prot_ID': gene_id, 'taxid': taxid, 'sequence': line.strip()+line.strip()})\n",
    "                            break\n",
    "\n",
    "            if i.endswith('.csv'):\n",
    "                with open(self.cog_source_path+i, 'r') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split(',')\n",
    "\n",
    "                        # add cog, gene, genbank ID, length, and assembly to json\n",
    "                        if i == 'cog-20.cog.csv':\n",
    "                            self.cog_json[i]['features'].append({'cog':  line[6], 'gene_id': line[0], 'genbank_prot_ID': line[2].replace(\n",
    "                                '.', '_'), 'gene_length': line[3], 'footprint_coords': line[12], 'assembly': line[1]})\n",
    "\n",
    "                        # add assembly, taxid, and phylum to json\n",
    "                        if i == 'cog-20.org.csv':\n",
    "                            self.cog_json[i]['features'].append(\n",
    "                                {'assembly': line[0], 'taxid': line[1], 'phylum': line[3]})\n",
    "\n",
    "                            # add assembly to clade_assemblies dict\n",
    "                            if line[3] not in self.clade_assemblies:\n",
    "                                self.clade_assemblies[line[3]] = []\n",
    "                            self.clade_assemblies[line[3]].append(line[0])\n",
    "\n",
    "            if i.endswith('.tab'):\n",
    "                with open(self.cog_source_path+i, 'r', encoding='cp1252') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split('\\t')\n",
    "                        \n",
    "                        # add cog, cog type, and product to json\n",
    "                        self.cog_json[i]['features'].append(\n",
    "                            {'cog': line[0], 'group': line[1], 'product': ' '.join(line[2:])})\n",
    "        if flatten:\n",
    "            self.flatten_json(self.cog_json)\n",
    "\n",
    "        if genomes:\n",
    "            return self.clade_assemblies\n",
    "\n",
    "    def write_json_to_file(self, flatten=True):\n",
    "        ''' write the flat json to a file '''\n",
    "        if flatten:\n",
    "            self.cog_json = self.flat_cog_json\n",
    "            f_name = 'cog-20-flat.json'\n",
    "        else:\n",
    "            f_name = 'cog-20.json'\n",
    "        with open(self.cog_source_path+f_name, 'w') as f:\n",
    "            json.dump(self.cog_json, f)  # indent=4)\n",
    "\n",
    "    def json_to_hdf(self):\n",
    "        ''' read the flat json file from directory and convert to hdf format'''\n",
    "        with pd.HDFStore(self.cog_source_path+'cog.h5') as store:\n",
    "            with open(self.cog_source_path+'cog-20-flat.json', 'r') as json_file:\n",
    "                for i, line in enumerate(json_file):\n",
    "                    try:\n",
    "                        flat_data = self.flatten_json(json.loads(line))\n",
    "                        df = pd.DataFrame.from_dict([flat_data])\n",
    "                        store.append('observations', df)\n",
    "                    except:\n",
    "                        print('error on line {}'.format(i))\n",
    "                        continue\n",
    "\n",
    "class GENOME_BATCH():\n",
    "    def __init__(self, assembly_list, outpath):\n",
    "        self.assembly_list = assembly_list\n",
    "        self.outpath = outpath\n",
    "\n",
    "    def get_NCBI_files(self, assembly, assembly_dir):\n",
    "        ''' get NCBI files for assembly '''\n",
    "\n",
    "        # split assembly up into thirds\n",
    "        assembly_num = assembly.split('.')[0].split('_')[1]\n",
    "        url = 'https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/{}/{}/{}/'.format(assembly_num[0:3], assembly_num[3:6], assembly_num[6:9])\n",
    "        \n",
    "        genbank_version = 'GCA_'+assembly.split('_')[1]\n",
    "        # get the directory listing in url and find the child directory\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for i in soup.find_all('a'):\n",
    "            if genbank_version in i.get('href'):\n",
    "                child_dir = i.get('href')\n",
    "                break\n",
    "        try:\n",
    "            # download the content from the child directory\n",
    "            r = requests.get(url+child_dir)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            for i in soup.find_all('a'):\n",
    "                if i.get('href').endswith('.gz'):\n",
    "                    # check if file already exists\n",
    "                    if not os.path.exists(assembly_dir+i.get('href')):\n",
    "                        with open(assembly_dir+i.get('href'), 'wb') as f:\n",
    "                            f.write(requests.get(url+child_dir+i.get('href')).content)\n",
    "        except:\n",
    "            print('error on assembly {}'.format(assembly), 'it\\'s probably becuase a child directory doesn\\'t exist')\n",
    "\n",
    "\n",
    "    def make_genome_dirctory(self):\n",
    "        ''' for every assembly in assembly list make a directory in outpath and get NCBI files '''\n",
    "        for clade in self.assembly_list:\n",
    "            clade_dir = self.outpath+clade+'/'\n",
    "            if not os.path.exists(clade_dir):\n",
    "                os.makedirs(clade_dir)\n",
    "            for assembly in self.assembly_list[clade]:\n",
    "                assembly_dir = clade_dir+assembly+'/'\n",
    "                if not os.path.exists(assembly_dir):\n",
    "                    os.makedirs(assembly_dir)\n",
    "                self.get_NCBI_files(assembly, assembly_dir)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cog_source_path = '/projects/lowelab/users/jsleavit/data/cogs/COG_ftp_files/'\n",
    "    clade_assemblies = COG_OBJECT(cog_source_path).build_cog_json(genomes=True)\n",
    "    \n",
    "    '''\n",
    "    # GENOME_BATCH failed after 40 min but probably doesn't need to be executed again since we have the genomes of interest\n",
    "    #GENOME_BATCH(clade_assemblies, '/projects/lowelab/users/jsleavit/data/cogs/cog_genomes/').make_genome_dirctory()\n",
    "    '''\n",
    "    # cog = COG_OBJECT(cog_source_path)\n",
    "    # cog.build_cog_json(flatten=True) # if want to convert to hdf, set flatten=True\n",
    "    # cog.write_json_to_file(flatten=True)\n",
    "    # cog.json_to_hdf() # doesn't appear to work...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29833a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align.Applications import MafftCommandline\n",
    "#from StringIO import StringIO\n",
    "from Bio import AlignIO\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class COG_STATS():\n",
    "    def __init__(self, cog_json_path, genbank_genome_path):\n",
    "        self.cog_json_path = cog_json_path\n",
    "        self.genbank_genome_path = genbank_genome_path\n",
    "\n",
    "\n",
    "    def read_json(self):\n",
    "        ''' brings in the cog json from the COG_OBJECT class '''\n",
    "        with open(self.cog_json_path, 'r') as f:\n",
    "            self.cog_json = json.load(f)\n",
    "    \n",
    "    def count_json(self, outpath):\n",
    "        ''' count the total number of genes for a given cog in each genome (warning: this took 1 hour to complete) '''\n",
    "\n",
    "        total_cog_counts = {}\n",
    "        for i in self.cog_json['cog-20.cog.csv']['features']:\n",
    "            phylum = self.get_phylum(i['assembly'])\n",
    "            self.get_cog_taxid_clan(i['cog'], phylum)\n",
    "            species = self.assembly_taxid[i['assembly'].split('.')[0].split('_')[1]]\n",
    "            total_cog_counts[(phylum,species,i['cog'],self.cog_clan[i['cog']])] = total_cog_counts.get((phylum,species,i['cog'],self.cog_clan[i['cog']]), 0) + 1\n",
    "\n",
    "\n",
    "        cog_count_json = {'features':[]}\n",
    "        for tax, cog_count in total_cog_counts.items():\n",
    "            p,s,co,cl = zip(tax)\n",
    "            token = {'species':s[0],'phylum':p[0],'cog':co[0],'clan':cl[0], 'gene_count':cog_count}\n",
    "            cog_count_json['features'].append(token)\n",
    "        \n",
    "        with open(outpath+'cog-20_count.json', 'w') as f:\n",
    "            json.dump(cog_count_json, f)\n",
    "            \n",
    "    \n",
    "    def get_clade_species(self, clade):\n",
    "        ''' helper function get all the cogs for a given clade with taxid identifier '''\n",
    "        self.clade = clade\n",
    "        self.clade_cogs = []\n",
    "        for i in self.cog_json['cog-20.org.csv']['features']:\n",
    "            if i['phylum'] == clade:\n",
    "                self.clade_cogs.append(i['taxid'])\n",
    "    \n",
    "    def get_cog_gene_ids(self, cog):\n",
    "        ''' helper function get all the gene ids for a given cog '''\n",
    "        self.cog = cog\n",
    "        self.cog_gene_ids = []\n",
    "        for i in self.cog_json['cog-20.cog.csv']['features']:\n",
    "            if i['cog'] == cog:\n",
    "                self.cog_gene_ids.append(i['genbank_prot_ID'])\n",
    "\n",
    "    def get_cog_taxid_clan(self, cog, clade):\n",
    "        ''' helper function get all the taxids by assembly for a cog and its clan '''\n",
    "\n",
    "        self.cog_clan = {}\n",
    "        self.assembly_taxid = {}\n",
    "\n",
    "        # cog-20.org.csv has the assembly info with taxid\n",
    "        for i in self.cog_json['cog-20.org.csv']['features']:\n",
    "            if i['phylum'] == clade:\n",
    "                short_assembly = i['assembly'].split('.')[0].split('_')[1]\n",
    "                self.assembly_taxid[short_assembly] = i['taxid']\n",
    "\n",
    "        # cog-20.def.tab has the clan info\n",
    "        for i in self.cog_json['cog-20.def.tab']['features']:\n",
    "            if i['cog'] == cog:\n",
    "                self.cog_clan[cog] = i['group']\n",
    "                \n",
    "    def get_phylum(self, assembly):\n",
    "        ''' helper function given the assembly get the respective phylum'''\n",
    "        for i in self.cog_json['cog-20.org.csv']['features']:\n",
    "            if i['assembly'].split('.')[0].split('_')[1] == assembly.split('.')[0].split('_')[1]:\n",
    "                return i['phylum']\n",
    "\n",
    "    def get_cog_seqs(self, clade=None, cog=None):\n",
    "        ''' get the sequences for a given cog '''\n",
    "\n",
    "        self.get_cog_gene_ids(cog)\n",
    "        self.get_clade_species(clade)\n",
    "\n",
    "        \n",
    "        cog_seqs = {}\n",
    "        for i in self.cog_json['cog-20.fa.gz']['features']:\n",
    "            if i['genbank_prot_ID'] in self.cog_gene_ids:\n",
    "                # check if the tax id is in any element of the clade list\n",
    "                if any('_'.join(i['taxid'].split(' ')[0:2]) in s for s in self.clade_cogs): \n",
    "                    cog_seqs[i['genbank_prot_ID']] = i['sequence']\n",
    "\n",
    "        # print brief summary of sequences for the cog\n",
    "        print('cog {} has {} sequences in clade {}'.format(cog, len(cog_seqs), self.clade))\n",
    "        self.cog_seqs = cog_seqs\n",
    "\n",
    "    def align_cog_seqs(self, outpath):\n",
    "        ''' take in the dictionary of geneID and seq aka cog_seqs and align them'''\n",
    "\n",
    "        # initialize alignment path for building hmm profile function later\n",
    "        self.alignment_path = outpath\n",
    "\n",
    "        # if outpath doesn't exist, create it\n",
    "        if not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "        \n",
    "        # align sequences and write as a stockholm file\n",
    "        for i in self.cog_seqs:\n",
    "            #write temp fasta file\n",
    "            seq = self.cog_seqs[i]\n",
    "            with open('temp.fa', 'a') as f:\n",
    "                f.write('>{}\\n{}\\n'.format(i, seq))\n",
    "\n",
    "\n",
    "        # align with mafft\n",
    "        mafft_cline = MafftCommandline(input='temp.fa')\n",
    "        stdout, stderr = mafft_cline()\n",
    "        # write alignment to temp file\n",
    "        with open('temp.fa', 'w') as f:\n",
    "            f.write(stdout)\n",
    "        # read stockholm file into biopython\n",
    "        alignment = AlignIO.parse(open('temp.fa'), 'fasta')\n",
    "        #stk_alignment = alignment.format('stockholm')\n",
    "        # remove all temp files\n",
    "        os.remove('temp.fa')\n",
    "        AlignIO.write(alignment, outpath+'alignment_{}_{}.sto'.format(self.cog,self.clade), 'stockholm')\n",
    "        print('alignment for {} in {} written to {}'.format(self.cog, self.clade, outpath))\n",
    "    \n",
    "    def build_hmm_profile(self,outpath):\n",
    "        ''' build hmm profile from stockholm alignment '''\n",
    "\n",
    "        # initialize hmm profile path for hmmsearch function later\n",
    "        self.hmm_path = outpath\n",
    "\n",
    "        # if outpath doesn't exist, create it\n",
    "        if not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "\n",
    "        for aln in os.listdir(self.alignment_path):\n",
    "            if aln.endswith('.sto'):\n",
    "                \n",
    "                # build hmm profile\n",
    "                os.system('hmmbuild {} {}'.format(outpath+aln.replace('.sto','.hmm'), self.alignment_path+aln) + ' > {}hmm_build_{}_{}.log'.format(outpath,self.cog,self.clade))\n",
    "\n",
    "    def remove_illegal_characters(self, genome_path):\n",
    "        ''' remove '-' from the sequences because hmmsearch can't handlge it '''\n",
    "        with gzip.open(genome_path, 'rt') as f:\n",
    "            # write back to file without the '-' character\n",
    "            with open(genome_path.replace('.gz',''), 'w') as g:\n",
    "                for line in f:\n",
    "                    if line.startswith('>'):\n",
    "                        g.write(line)\n",
    "                    else:\n",
    "                        g.write(line.replace('-',''))\n",
    "\n",
    "    def search_genome_cds(self, outpath,retrieve_only=False):\n",
    "        ''' search genome cds with hmm profile '''\n",
    "\n",
    "        if retrieve_only == True:\n",
    "            self.hmmsearch_path = outpath\n",
    "            return\n",
    "\n",
    "        # initialize hmmsearch path for parsing stats later\n",
    "        self.hmmsearch_path = outpath\n",
    "\n",
    "        # if outpath doesn't exist, create it\n",
    "        if not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "        \n",
    "        # get all the hmm profiles\n",
    "        hmm_files = {}\n",
    "        for hmm in os.listdir(self.hmm_path):\n",
    "            if hmm.endswith('.hmm'):\n",
    "                if hmm.split('_')[1] not in hmm_files:\n",
    "                    hmm_files[hmm.split('_')[2].replace('.hmm', '')] = {}\n",
    "                    hmm_files[hmm.split('_')[2].replace('.hmm', '')][hmm.split('_')[1]] = self.hmm_path+hmm\n",
    "\n",
    "        # get all the genome cds \n",
    "        genome_cds = {}\n",
    "        for genome in os.listdir(self.genbank_genome_path+self.clade):\n",
    "            for file in os.listdir(self.genbank_genome_path+self.clade+'/'+genome):\n",
    "                if '_cds.faa.gz' in file:\n",
    "                    self.remove_illegal_characters(self.genbank_genome_path+self.clade+'/'+genome+'/'+file)\n",
    "                    genome_cds[genome] = self.genbank_genome_path+self.clade+'/'+genome+'/'+file.replace('.gz','')\n",
    "        n_genomes = []       \n",
    "        n = 0\n",
    "        for clade in os.listdir(self.genbank_genome_path):\n",
    "            if clade == self.clade:\n",
    "                for genome in os.listdir(self.genbank_genome_path+clade):\n",
    "\n",
    "                    # search hmm profile\n",
    "                    if genome in genome_cds:\n",
    "\n",
    "                        # only search if results file doesn't exist\n",
    "                        if not os.path.exists(outpath+'{}results_{}_{}_{}.hmmsearch_.out'.format(outpath, self.cog, self.clade, '_'.join(genome.split('.')))):\n",
    "                            os.system('hmmsearch -o {}results_{}_{}_{}.hmmsearch.out {} {} '.format(outpath, self.cog, self.clade, '_'.join(genome.split('.')), hmm_files[clade][self.cog], genome_cds[genome]))\n",
    "                            print('hmmsearch of {}  {}  in {} written to {}'.format(self.clade, self.cog, genome, outpath))\n",
    "\n",
    "                    else:\n",
    "                            if not os.path.exists(outpath+'{}results_{}_{}_{}.hmmsearch.out'.format(outpath, self.cog, self.clade, '_'.join(genome.split('.')))):\n",
    "                                try:\n",
    "                                    os.system('hmmsearch -o {}results_{}_{}_{}.hmmsearch.out {} {} '.format(outpath, self.cog, self.clade, '_'.join(genome.split('.')), hmm_files[clade][self.cog], genome_cds['GCA_'+genome.split('_')[1]] ))\n",
    "                                except:\n",
    "                                    print('no genome cds for {}'.format(genome))\n",
    "                                    n_genomes.append(genome)\n",
    "                                    n += 1\n",
    "\n",
    "        print('{} genomes have no cds file'.format(n), '{}'.format(n_genomes) if n_genomes != [] else '')\n",
    "        print('{} hmmsearch failed'.format(f))\n",
    "\n",
    "    def count_my_hmmsearch(self):\n",
    "        ''' parses the hmm search file to get counts of cogs  '''\n",
    "\n",
    "        # count number of hits per genome\n",
    "        cog, clade = None, None\n",
    "        hmmsearch_results = {'features':[]}\n",
    "        for hmmsearch in os.listdir(self.hmmsearch_path+'tblout_results/'):\n",
    "            if hmmsearch.endswith('.hmmsearch.out'):\n",
    "                assembly = hmmsearch.split('.')[0].split('_')[-2] # might want this later to access the species name\n",
    "                cog, clade  = hmmsearch.split('.')[0].split('_')[1], hmmsearch.split('.')[0].split('_')[2]\n",
    "                self.get_cog_taxid_clan(cog,clade)\n",
    "                if hmmsearch.endswith('.hmmsearch.out'):\n",
    "                    count = 0\n",
    "                    with open(self.hmmsearch_path+'tblout_results/'+hmmsearch, 'r') as f:\n",
    "                        for line in f:\n",
    "                            \n",
    "                            # each entry is on a line without a # in front\n",
    "                            if not line.startswith('#'):\n",
    "                                count += 1\n",
    "\n",
    "                        hmmsearch_results['features'].append({'species': self.assembly_taxid[assembly] , 'phylum': clade, 'cog': cog, 'clan': self.cog_clan[cog], 'gene_count': count})\n",
    "        self.hmmsearch_results = hmmsearch_results\n",
    "\n",
    "    def compare_cog_counts(self, cog_count_path):\n",
    "        ''' compare my hmmsearch results to the COG database '''\n",
    "        # load the cog count json\n",
    "\n",
    "\n",
    "        with open(cog_count_path, 'r') as f:\n",
    "            cog_count = json.load(f)\n",
    "\n",
    "\n",
    "        my_clades = set()\n",
    "        my_cogs = set()\n",
    "        my_species = set()\n",
    "        species_cog_count = {}\n",
    "        for clade in self.hmmsearch_results['features']:\n",
    "            my_clades.add(clade['phylum'])\n",
    "            my_cogs.add(clade['cog'])\n",
    "            my_species.add(clade['species'])\n",
    "            if clade['species'] not in species_cog_count:\n",
    "                species_cog_count[clade['species']] = clade['gene_count']\n",
    "        \n",
    "        cog, hom = 'COG0590', 'HOM0590'\n",
    "        sub_cog_count = {}\n",
    "        for i in cog_count['features']:\n",
    "            if i['cog'] == cog:\n",
    "                sub_cog_count[i['species']] = i['gene_count']\n",
    "\n",
    "        # add the cog counts to comparision dict\n",
    "        comparison = {'features':[]}\n",
    "        for i in self.hmmsearch_results['features']:\n",
    "            if i['species'] in sub_cog_count:\n",
    "                if i['gene_count'] >= sub_cog_count[i['species']]:\n",
    "                    comparison['features'].append({'species': i['species'], 'phylum': i['phylum'], 'cog': hom, 'clan': i['clan'], 'gene_count': i['gene_count']})\n",
    "                    comparison['features'].append({'species': i['species'], 'phylum': i['phylum'], 'cog': cog, 'clan': i['clan'], 'gene_count': sub_cog_count[i['species']]})\n",
    "        with open(self.hmmsearch_path+'comparison_hmmsearch_NCBI_COG_COG0590_no_zero.json', 'w') as f:\n",
    "            json.dump(comparison, f)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cog_object_path = '/projects/lowelab/users/jsleavit/data/cogs/COG_ftp_files/cog-20.json'\n",
    "    genbank_genome_path = '/projects/lowelab/users/jsleavit/data/cogs/cog_genomes/'\n",
    "    cog_counts_path = '/projects/lowelab/users/jsleavit/data/cogs/COG_ftp_files/cog-20_count.json'\n",
    "    CS = COG_STATS(cog_object_path, genbank_genome_path)\n",
    "    CS.read_json()\n",
    "    #CS.count_json('/projects/lowelab/users/jsleavit/data/cogs/COG_ftp_files/') probably don't ever need to run this unless adding something to the json\n",
    "\n",
    "    ''' run this chunk for doing initial hmmsearch, if hasn't been done before '''\n",
    "    # CS.get_cog_seqs(clade='EURYARCHAEOTA', cog = 'COG0590')\n",
    "    # CS.align_cog_seqs('/projects/lowelab/users/jsleavit/data/cogs/cog_alignments/')\n",
    "    # CS.build_hmm_profile('/projects/lowelab/users/jsleavit/data/cogs/cog_hmms/')\n",
    "    # CS.search_genome_cds('/projects/lowelab/users/jsleavit/data/cogs/cog_hmmsearch/')\n",
    "\n",
    "    ''' run this chunk if the hmmsearch already has been done '''\n",
    "    CS.search_genome_cds('/projects/lowelab/users/jsleavit/data/cogs/cog_hmmsearch/', retrieve_only=True)\n",
    "    CS.count_my_hmmsearch()\n",
    "    CS.compare_cog_counts(cog_counts_path)\n",
    "  \n",
    "\n",
    "\n",
    "    ''' needs work for multiple clades '''\n",
    "    # clades = ['EURYARCHAEOTA', 'THAUMARCHAEOTA', 'CRENARCHAEOTA', 'OTHER\\ ARCHAEA']\n",
    "    # #clades = [ 'CRENARCHAEOTA', 'OTHER\\ ARCHAEA']\n",
    "    # statue = True\n",
    "    # for clade in clades:\n",
    "    #     CS.get_cog_sequences(clade=clade, cog = 'COG0590')\n",
    "    #     #CS.get_cog_sequences(clade='EURYARCHAEOTA', cog='COG0590')\n",
    "    #     status=CS.align_cog_seqs(outpath='/projects/lowelab/users/jsleavit/data/cogs/cog_alignments/')\n",
    "    #     if status:\n",
    "    #         CS.build_hmm_profile(outpath='/projects/lowelab/users/jsleavit/data/cogs/cog_hmms/')\n",
    "    #         CS.search_genome_cds(outpath='/projects/lowelab/users/jsleavit/data/cogs/cog_hmmsearch/')\n",
    "    #     else:\n",
    "    #         print(clade+ 'doesn\\'t have seed for alignment and hmmsearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d72ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "\n",
    "''' Collect the top hits from each branch  and use hmmalign to align them '''\n",
    "class FastAreader():\n",
    "    def __init__(self, fname=''):\n",
    "        ''' contructor: saves attribute fname '''\n",
    "        self.fname = fname\n",
    "\n",
    "    def doOpen(self):\n",
    "        if self.fname == '':\n",
    "            return sys.stdin\n",
    "        else:\n",
    "            return open(self.fname)\n",
    "\n",
    "    def readFasta(self):\n",
    "        header = ''\n",
    "        sequence = ''\n",
    "\n",
    "        with self.doOpen() as fileH:\n",
    "            header = ''\n",
    "            sequence = ''\n",
    "\n",
    "            # skip to first fasta header\n",
    "            line = fileH.readline()\n",
    "            while not line.startswith('>'):\n",
    "                line = fileH.readline()\n",
    "            header = line[1:].rstrip()\n",
    "\n",
    "            for line in fileH:\n",
    "                if line.startswith('>'):\n",
    "                    yield header, sequence\n",
    "                    header = line[1:].rstrip()\n",
    "                    sequence = ''\n",
    "                else:\n",
    "                    sequence += ''.join(line.rstrip().split()).upper()\n",
    "\n",
    "        yield header, sequence\n",
    "\n",
    "class HMM_STATS():\n",
    "    def __init__(self, hmmsearch_path, hmm_path, hmmalign_path, cog_genome_path,clade,tree_path):\n",
    "        self.hmmsearch_path = hmmsearch_path\n",
    "        self.hmm_path = hmm_path\n",
    "        self.hmmalign_path = hmmalign_path\n",
    "        self.cog_genome_path = cog_genome_path+clade+'/'\n",
    "\n",
    "        self.clade = clade\n",
    "\n",
    "        # read in assembly to genome dictionary\n",
    "        self.assembly_to_genome = json.load(open(cog_genome_path+'assembly_to_species.json', 'r'))\n",
    "\n",
    "        # read in taxonomic assignment dictionary\n",
    "        self.taxa_assignment = json.load(open(cog_genome_path+'species_taxonomic_assignment.json', 'r'))\n",
    "\n",
    "        # read in tree\n",
    "        self.tree = Tree(tree_path)\n",
    "    \n",
    "\n",
    "    def collect_top_hits(self):\n",
    "        ''' collect the gene name of the top hit from each branch '''\n",
    "        top_hit_genes = {}\n",
    "        for hmm in os.listdir(self.hmmsearch_path):\n",
    "            assembly = hmm.split('.')[0].split('_')[-2]\n",
    "            if self.clade in hmm and hmm.endswith('.hmmsearch.out'):\n",
    "                with open(self.hmmsearch_path+hmm, 'r') as f:\n",
    "                    first = True\n",
    "                    for line in f:\n",
    "                        # if line doesn't start with #, then it is a hit\n",
    "                        if not line.startswith('#'):\n",
    "                            if first:\n",
    "                                gene = line.split()[0]\n",
    "                                if assembly not in top_hit_genes:\n",
    "                                    top_hit_genes[assembly] = []\n",
    "                                top_hit_genes[assembly].append(gene)\n",
    "                                first = False\n",
    "        self.top_hit_genes = top_hit_genes\n",
    "\n",
    "    def collect_sequences_from_genome(self):\n",
    "        ''' use the top hit gene dict to extract the genes '''\n",
    "\n",
    "        # if hmm_align_path doesn't exist, make it\n",
    "        if not os.path.exists(self.hmmalign_path):\n",
    "            os.mkdir(self.hmmalign_path)\n",
    "\n",
    "        for assembly in os.listdir(self.cog_genome_path):\n",
    "            short_name_ass =assembly.split('.')[0].split('_')[1]\n",
    "            species = self.assembly_to_genome[short_name_ass]\n",
    "            if short_name_ass in self.top_hit_genes:\n",
    "                for file in os.listdir(self.cog_genome_path+assembly):\n",
    "        \n",
    "                    if file.endswith('_cds.faa'):\n",
    "                        FA = FastAreader(self.cog_genome_path+assembly+'/'+file)\n",
    "                        for head, seq in FA.readFasta():\n",
    "                    \n",
    "                            if head.split()[0] in self.top_hit_genes[short_name_ass]:\n",
    "                                # get the locus tag from head\n",
    "                                locus_tag = [x.split('=')[1].strip('[').strip(']') for x in head.split() if 'locus_tag' in x]\n",
    "                                protein_id = [x.split('=')[1].strip('[').strip(']') for x in head.split() if 'protein_id' in x]\n",
    "                                if len(locus_tag) == 1:\n",
    "                                    locus_tag = locus_tag[0]\n",
    "                                else:\n",
    "                                    locus_tag = protein_id[0]\n",
    "                                head = species+'_'+locus_tag\n",
    "                                with open(self.hmmalign_path+head+'.faa', 'w') as f:\n",
    "                                    f.write('>'+head+'\\n'+seq+'\\n')\n",
    "                \n",
    "         \n",
    "                  \n",
    "    def align_sequences(self, phylum):\n",
    "        ''' use the hmmalign to align the sequences '''\n",
    "\n",
    "\n",
    "        # if hmm_align_path doesn't exist, make it\n",
    "        if not os.path.exists(self.hmmalign_path):\n",
    "            os.mkdir(self.hmmalign_path)\n",
    "\n",
    "        # concatenate all the sequences into one file\n",
    "        with open(self.hmmalign_path+phylum+'_top_hits.faa', 'w') as f:\n",
    "            for file in os.listdir(self.hmmalign_path):\n",
    "                if file.endswith('.faa'):\n",
    "                    with open(self.hmmalign_path+file, 'r') as f2:\n",
    "                        for line in f2:\n",
    "                            f.write(line)\n",
    "\n",
    "        for hmm in os.listdir(self.hmm_path):\n",
    "            if hmm.endswith('.hmm'):\n",
    "               \n",
    "                # if the hmm file is for the phylum we are looking at\n",
    "                if hmm.split('.')[0].split('_')[-1] == phylum:\n",
    "\n",
    "                    os.system('hmmalign --trim --amino --informat FASTA -o '+self.hmmalign_path+phylum+'_top_hits.sto '+self.hmm_path+hmm+' '+ self.hmmalign_path+phylum+'_top_hits.faa ')#+self.hmmalign_path+'ALL_EURYARCHAEOTA_TOP_HITS.sto')\n",
    "                    print('hmm profile: '+hmm+' aligned with '+phylum+' sequences')\n",
    "    def collect_tree_genera(self):\n",
    "        ''' collect the genera for the r_tree '''\n",
    "\n",
    "        species2genus = {}\n",
    "        for i in self.tree.iter_leaves():\n",
    "            leaf_name = ' '.join(i.name.split('_')[0:2])\n",
    "            if leaf_name in self.taxa_assignment:\n",
    "                genus = self.taxa_assignment[leaf_name][-3]\n",
    "\n",
    "                if genus is None:\n",
    "                    # use previous iteration genus if genus is None ( this is safe because we are iterating through the tree in topographical order of relationships)\n",
    "                    genus = prev_genus\n",
    "                    species2genus[leaf_name] = genus\n",
    "\n",
    "                species2genus[leaf_name] = genus\n",
    "                prev_genus = genus\n",
    "\n",
    "        self.species2genus = species2genus\n",
    "        \n",
    "\n",
    "\n",
    "    def subset_top_hits_by_genera(self):\n",
    "        ''' use the species2genus dict to pull species by genera from the fasta file '''\n",
    "\n",
    "        # read in the fasta file\n",
    "        genera_found = set()\n",
    "        FA = FastAreader(self.hmmalign_path+self.clade+'_top_hits.faa')\n",
    "        for head, seq in FA.readFasta():\n",
    "            species = ' '.join(head.split('_')[0:2])\n",
    "            if species in self.species2genus:\n",
    "                genus = self.species2genus[species]\n",
    "                genera_found.add(genus)\n",
    "                # open a directory for the genus if it doesn't exist and write a new fasta file for the genus\n",
    "                if not os.path.exists(self.hmmalign_path+genus):\n",
    "                    os.mkdir(self.hmmalign_path+genus)\n",
    "                with open(self.hmmalign_path+genus+'/'+head+'.faa', 'w') as f:\n",
    "                    f.write('>'+head+'\\n'+seq+'\\n')\n",
    "                \n",
    "        self.genera_found = genera_found\n",
    "\n",
    "    def hmm_align_genera(self):\n",
    "        ''' if there is more than one sequence use the hmm align with the respective phylum '''\n",
    "    \n",
    "        for genus in self.genera_found:\n",
    "            # if there is more than one sequence in the genus\n",
    "            if len(os.listdir(self.hmmalign_path+genus)) > 1:\n",
    "                # concatenate all the sequences into one file\n",
    "                with open(self.hmmalign_path+genus+'/'+genus+'_top_hits.faa', 'w') as f:\n",
    "                    for file in os.listdir(self.hmmalign_path+genus):\n",
    "                        if file.endswith('.faa'):\n",
    "                            with open(self.hmmalign_path+genus+'/'+file, 'r') as f2:\n",
    "                                for line in f2:\n",
    "                                    f.write(line)\n",
    "\n",
    "                for hmm in os.listdir(self.hmm_path):\n",
    "                    if hmm.endswith('.hmm'):\n",
    "                        # if the hmm file is for the phylum we are looking at (this is still using the phylum level profile)\n",
    "                        if hmm.split('.')[0].split('_')[-1] == self.clade: \n",
    "\n",
    "                            os.system('hmmalign --trim --amino --informat FASTA -o '+self.hmmalign_path+genus+'/'+genus+'_top_hits.sto '+self.hmm_path+hmm+' '+ self.hmmalign_path+genus+'/'+genus+'_top_hits.faa ')    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hmmsearch_path = '/projects/lowelab/users/jsleavit/data/cogs/cog_hmmsearch/tblout_results/' # has the hmmsearch results\n",
    "    hmm_path = '/projects/lowelab/users/jsleavit/data/cogs/cog_hmms/' # has the hmm profiles for each cog\n",
    "    hmmalign_path = '/projects/lowelab/users/jsleavit/data/cogs/cog_hmmalign/' # will have the hmmalign results\n",
    "    cog_genome_path = '/projects/lowelab/users/jsleavit/data/cogs/cog_genomes/' # has the genomes for each species\n",
    "    tree_path = '/projects/lowelab/users/jsleavit/data/rna_db_out/archaea/a_16S_23S_combined_names_with_cogs.sina.aligned.fa.final_tree.nw'\n",
    "\n",
    "    HS = HMM_STATS(hmmsearch_path, hmm_path, hmmalign_path, cog_genome_path, 'EURYARCHAEOTA', tree_path)\n",
    "\n",
    "\n",
    "    ''' this chunk collects the top hits from hmmsearch results, finds the hit in the organisms CDS genome, and aligns them all together '''\n",
    "    HS.collect_top_hits()\n",
    "    HS.collect_sequences_from_genome()\n",
    "    HS.align_sequences('EURYARCHAEOTA')\n",
    "\n",
    "\n",
    "    ''' subset the top hits and  group specific genera based on their phylogeny and then align them individually '''\n",
    "\n",
    "    # need to read in the tree\n",
    "    # then categorize the species in the tree from the species_taxonomic_assignment.json in cog_genoms folder\n",
    "    HS.collect_tree_genera()\n",
    "    HS.subset_top_hits_by_genera()\n",
    "    HS.hmm_align_genera()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # HS.get_top_hits()\n",
    "    # HS.align_top_hits()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('python_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31760f4f0b3ac7714abb2ef585177fad655bb0e1a510dfb297d2f95018f47037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
